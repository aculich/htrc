{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HathiTrust Research Center (HTRC)\n",
    "\n",
    "The [HathiTrust Digital Library](https://www.hathitrust.org/) contains over 14 million volumes scanned from academic libraries around the world (primarily in North America). The [HathiTrust Research Center](https://analytics.hathitrust.org/) allows researchers to access almost all of those texts in a few different modes for computational text analysis. \n",
    "\n",
    "This notebook will walk us through getting set-up to analyze [HTRC Extracted Features](https://wiki.htrc.illinois.edu/display/COM/Extracted+Features+Dataset) for volumes in HathiTrust in a Jupyter/Python environment. *Extracted Features* are currently (as of April 2017) the most robust way to access in-copyright works from the HT Library for computational analysis. \n",
    "\n",
    "For more information on HTRC: \n",
    "* [Library text mining guide page on HTRC](http://guides.lib.berkeley.edu/c.php?g=491766&p=3381443)\n",
    "* [Programming Historian's Text Mining in Python through the HTRC Feature Reader](http://programminghistorian.org/lessons/text-mining-with-extracted-features)\n",
    "\n",
    "## Installation\n",
    "\n",
    "To start we'll need to install a few things:\n",
    "* Install the *HTRC Feature Reader* to work with Extracted Features: \n",
    "```\n",
    "conda install -c htrc htrc-feature-reader\n",
    "``` \n",
    "or\n",
    "```\n",
    "pip install htrc-feature-reader\n",
    "pip install matplotlib jupyter\n",
    "```\n",
    "* Install Rsync to download Extracted Features from HathiTrust:\n",
    "\n",
    "  * For linux:\n",
    "```\n",
    "yum -y install rsync\n",
    "```\n",
    "  * For mac:\n",
    "```\n",
    "brew tap homebrew/dupes\n",
    "brew install rsync\n",
    "```\n",
    "\n",
    "## Add volumes from HTRC\n",
    "\n",
    "### Finding Volume IDs in HathiTrust\n",
    "\n",
    "To build your own corpus, you will need to find the volume ID for each volume you'd like to include from the [HathiTrust Library](https://www.hathitrust.org/).\n",
    "\n",
    "* Search for your book, and copy the URL from the *Limited (Search Only)* or *Full View* links under the work. <img src=\"files/judith-butler-ht.png\">\n",
    "* The final string of characters after the final / is your volume ID\n",
    "* For example, mdp.39015070698322 is the volume ID for \"https://hdl.handle.net/2027/mdp.39015070698322\"\n",
    "\n",
    "### Rsync the volumes\n",
    "\n",
    "Now that you've identified the volumes you'd like to use, you can run Rsync to pull down their Extracted Features for use with the HTRC Feature Reader.\n",
    "\n",
    "First, make your way to the directory where you plan to do your work.\n",
    "\n",
    "If you're planning to analyze only a few volumes you can use the following command, replacing {{volume_id}} with your own:\n",
    "```\n",
    "htid2rsync {{volume_id}} | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ local-folder/\n",
    "```\n",
    "\n",
    "If you have a file of volume ids, one per line, use --from-file filename, or just -f filename.\n",
    "```\n",
    "htid2rsync --f volumeids.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ local-folder/\n",
    "```\n",
    "\n",
    "It's also possible to work with the entire library (4TB, so beware):\n",
    "```\n",
    "rsync -rv data.analytics.hathitrust.org::features/ .\n",
    "```\n",
    "\n",
    "Or to use existing lists of public-domain [fiction](http://data.analytics.hathitrust.org/genre/fiction_paths.txt), [drama](http://data.analytics.hathitrust.org/genre/drama_paths.txt), and [poetry](http://data.analytics.hathitrust.org/genre/poetry_paths.txt) (Underwood 2014).\n",
    "\n",
    "## Working with the Extracted Features\n",
    "\n",
    "In the example, below, we'll work with five volumes of the *Congressional Record* using IDs provided by HathiTrust. I've listed those IDs (randomly selected from a much larger list) in the file, cr_ids_5.txt.\n",
    "\n",
    "Let's download the extracted features for each of those five volumes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "htid2rsync --f cr_ids_5.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ local-folder/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the examples of code below are taken directly, or adapted, from the [Programming Historian tutorial](http://programminghistorian.org/lessons/text-mining-with-extracted-features) or the [FeatureReader's Readme.md file](https://github.com/htrc/htrc-feature-reader).\n",
    "\n",
    "You'll notice, from the output above, that the content for each volume is stored in a compressed JSON file, in a rather lengthy file directory. We can import and initialize FeatureReader with file paths pointing to the six JSON files (using the paths from the output above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader\n",
    "import os\n",
    "paths = [os.path.join('local-folder', 'chi/pairtree_root/50/17/03/73/50170373/chi.50170373.json.bz2'), \n",
    "         os.path.join('local-folder', 'njp/pairtree_root/32/10/10/76/47/62/15/32101076476215/njp.32101076476215.json.bz2'), \n",
    "         os.path.join('local-folder', 'uc1/pairtree_root/$c/22/51/89/$c225189/uc1.$c225189.json.bz2'), \n",
    "         os.path.join('local-folder', 'uiuo/pairtree_root/ar/k+/=1/39/60/=t/7s/n1/rk/4f/ark+=13960=t7sn1rk4f/uiuo.ark+=13960=t7sn1rk4f.json.bz2'), \n",
    "         os.path.join('local-folder', 'uva/pairtree_root/x0/30/51/56/78/x030515678/uva.x030515678.json.bz2')]\n",
    "fr = FeatureReader(paths)\n",
    "for vol in fr.volumes():\n",
    "    print(vol.id, vol.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are pretty repetitive titles, which makes sense since they are from different volumes of the same serial title, though we can see that a volume of the *Congressional Globe*, an earlier version of the CR, is also included.\n",
    "\n",
    "Let's try to pull out some more metadata about these titles, using the [Volume object](http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html#htrc_features.feature_reader.Volume) in FeatureReader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show the HT URL, year, and page count for each volume\n",
    "for vol in fr.volumes():\n",
    "    print(\"URL: %s Year: %s Page count: %s \" % (vol.handle_url, vol.year, vol.page_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#where were these volumes scanned?\n",
    "for vol in fr.volumes():\n",
    "    print(\"Source institution: %s \" % (vol.source_institution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vol = fr.first()\n",
    "tokens = vol.tokens_per_page()\n",
    "# Show just the first few rows, so we can look at what it looks like\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "tokens.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at some specific pages, using the [Page object in FeatureReader](http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html#htrc_features.feature_reader.Page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for page in vol:\n",
    "    # Same as `for page in vol.pages()`\n",
    "    i += 1\n",
    "    if i >= 300:\n",
    "        break\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The body has %s lines, %s empty lines, and %s sentences\" % (page.line_count(),\n",
    "                                                                   page.empty_line_count(),\n",
    "                                                                   page.sentence_count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#look at the first 20 tokens on the page\n",
    "print(page.tokenlist()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = page.tokenlist(section=\"header\", case=False, pos=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = page.tokenlist()\n",
    "# Slicing on Multiindex: get all Signular or Mass Nouns (NN)\n",
    "idx = pd.IndexSlice\n",
    "nouns = df.loc[idx[:,:,:,'NN'],]\n",
    "print(nouns[:3])\n",
    "print(\"With index reset: \")\n",
    "print(nouns.reset_index()[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_vol_token_counts = vol.tokenlist(pos=False, case=False)\n",
    "print(all_vol_token_counts.loc[idx[:,'body', 'she'],][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = vol.term_page_freqs()\n",
    "print(a.loc[10:15,['the','and','is','he', 'she']])\n",
    "a = vol.term_page_freqs(page_freq=False)\n",
    "print(a.loc[10:15,['the','and','is', 'he', 'she']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
